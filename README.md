# PTV - Process Tree Verifier
This is the Github of the Process Tree Verification tool developed for the *Removed for Review* Paper as part of the *removed for Review* Project. The Process Tree Verifier is a Subscribtion based Rest Service that can be used to verify regulatory requirements on processes represented as process trees in the [cpee](https://www.cpee.org)-tree format.

This readme contains instructins on how to use the developed tool (A) with existing process to directly see the functionality, (B) with new processes / requirements to show how it affects the process of process modeling, (C) with a small testing script locally (D) with a locally deployed copy of the PTV for future development.

The prompts used for generating ASTs out of Natural Language / Textual Process Descriptions, are in the ExtractionPrototype Directory.

The dataset used for evaluation is in the CompositeDataset directory. The processes are given as xml files that already contain the requiremend ASTs so they can be loaded into the CPEE and verified as described in (B).

The complete Documentation can be found in a [Google Doc](https://docs.google.com/document/d/1zmmlLmjx7WXjEr13STYjuhUX3BA8nUhPKcK7uclMJtI/edit?usp=sharing).
In case this link breaks for any reason during the Review phase it is also in documentation.txt.

For Reviewers it is only possible to verify processes using (C) and (D) since, (A) and (B) would reveal information about the authors
## (A) Testing with existing Processes

Testing with existing processes reveals information that can be used to identify the authors so it is not possible for the Reviewer Version.
The following is an example of the compliance log generated by verifying the Running example used in the Paper with additional requirements,
using the webservice interface to the PTV.
![image](https://github.com/user-attachments/assets/56bb9ad8-c078-4ce4-981e-8a717022773b)


## (B) Adding the subscription to a new Process

Compliance Log cannot be returned without revealing information that can be used to identify the authors so it is not possible for the Reviewer Version.
The following steps only show how you can setup a process to connect a subscriber to highlight that it is very easy to connect new processes with the subscriber, but the actual verification can again not be shown

If you want to try out the verification yourself you can also create a new model on the CPEE and connect it to the compliance subscriber. For this follow the following steps:

Use the CPEE functionality "save testset" to download the xml testset.

![image](https://github.com/user-attachments/assets/b6695050-f6a7-498c-a839-a46f269ee178)

Add the Compliance Subscriber to the testset by copy pasting the following into the xml. (Can also check out any of the TestSets linked above for an example of a .XML testset with the subscriber added, by downloading it using the same "save testset" functionality.

<subscriptions>
<subscription id="_compliance" url="https://>double blind</compliance/Subscriber">
<topic id="description">
<event>change</event>
</topic>
</subscription>
</subscriptions>


Now use the "load testset" button (seen above) to load the edited xml into the process. Save the model for safety.
To actually verify anything you have to still add compliance requirements. 
The compliance requirements are sent to the subscriber via the Attributes fields.
Accordingly, add any requirements encoded as an AST you want into it like so:

![image](https://github.com/user-attachments/assets/f1901d4b-7f8c-4973-8905-df37c445a63c)

For a complete overview of all verification methods you can check the source code in the python\_code dictionary or the regularly updated Documentation at 

Now any change in the process will seend a message to the subscriber. The resulting compliance log can not be sent to reviewers without revealing author information. 

## (C) Local Testing Scripts
Complete local deployment requires setting up a server / configuring a firewall. In order to simplify local testing for users that do not want to spend that time, but do want to test the PTV locally we prepared a simple script interface for the PTV. To use the testing script you first have to clone the repository and install dependencies. Intructions were tested on a fresh Fedora 43 installation, but should work on other distributions and Windows/Mac as well.

1. Clone the Repository `git pull xxx`
2. Navigate to the python\_code dictionary `cd python_code`
3. (This was optional on Fedora): Install all dependencies `pip install -r requirements.txt`
4. On Linux: Launch the testing script `python3 test_script.py ../RunningExample/Running_Example.xml`
5. On Windows: Launch the testing script `python3 test_script.py ..\RunningExample\Running_Example.xml`

The above example tests the running example process also used throughout the paper, but you can also verify any other process xml found in the Composite Dataset, the two survey processes as well as any processes created in the process hub (as long as you add requirements to them)


## (D) Custom Deployment
Finnally, you can also deploy the PTV locally, which we recommend in case you want to add additional functionality or simply run different tests.
The CPEE side would be handled the same as before, with the only change being that the url needs to point towards your own endpoint. We recommend using a server such as nginx to forward the port (default is port 9321, can be changed in python_code/compliancesub.py) towards an URL. These instructions were tested on a fresh Fedora 43 Installation, but should work on different systems as well. Depending on your distribution or Windows/Mac you might have to install additional packages such as python3 and pip (any current version should do, all required packages are standard libraries).

To actually launch the project follow these steps:

1. Clone the Repository `git pull xxx`
2. Navigate to the python\_version dictionary `cd python_version`
3. Install all dependencies `pip install -r requirements.txt`
4. Launch the Application using `python3 compliancesub.py` (launches as a daemon, short explanation below)
5. Either use the endpoint at local host (127.0.0.1:9321) or set up port forwarding by setting up your firewall and webserver (we recommend using nginx and also setting up let's encrypt)
6. End the Daemon after using by executing `python3 compliancesub.py` again

compliancessub.py also contains a little script (def run_server():) to ensure that the subscriber is started as a daemon. We are unsure how this will work on Windows/Mac so if you encounter any issues on these systems you can remove that codepiece and use the "normal" way to run fastapi rest services using `uvicorn.run compliancesub:app port=9321`. If you still encounter any issues on these systems you can contact us or just try out the example scripts presented in Section (C)

   


